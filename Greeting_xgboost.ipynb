{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5m7hA7GL9mH",
        "outputId": "ec22da03-885d-492b-80c7-e1eb87f3a327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaVEWh5NPmf-",
        "outputId": "3fc4b126-95a6-460e-bb94-a135abe3ba3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAwA9LUTRX4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a080ff5-3f8a-4535-c20c-fc0068dfea46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0029.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0030.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0031.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0032.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0037.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0038.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0039.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0089.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0090.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_0091.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9914.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9915.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9916.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9917.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9954.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9955.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9956.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9957.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9982.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9983.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Hello/MVI_9984.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0046.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0047.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0048.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0049.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0050.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0050_ (1).MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0050_ (2).MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0051.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0101.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0102.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_0103.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9936.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9937.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9938.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9939.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9972.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9973.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9974.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9975.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9994.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9995.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good afternoon/MVI_9996.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0042.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0043.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0044.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0045.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0046.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0047.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0048.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0098.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0099.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_0100.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9932.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9933.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9934.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9935.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9968.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9969.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9970.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9971.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9991.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9992.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Good Morning/MVI_9993.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0037.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0038.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0039.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0040.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0043.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0044.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0045.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0095.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0096.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_0097.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9923.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9924.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9925.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9926.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9963.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9964.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9965.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9966.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9988.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9989.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/Alright/MVI_9990.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0033.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0034.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0035.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0036.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0040.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0041.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0042.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0092.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0093.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_0094.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9918.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9919.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9920.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9921.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9959.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9960.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9961.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9962.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9985.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9986.MOV...\n",
            "Processing /content/drive/MyDrive/AISC/Greetings/How are you/MVI_9987.MOV...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- Augmentation Functions ----------\n",
        "\n",
        "def center_crop(frame, crop_fraction=0.4):\n",
        "    \"\"\"\n",
        "    Crop the center of the frame to the given fraction of original size.\n",
        "    For instance, crop_fraction=0.4 returns a region that is 40% of the width and height.\n",
        "    \"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "    new_w = int(w * crop_fraction)\n",
        "    new_h = int(h * crop_fraction)\n",
        "    start_x = (w - new_w) // 2\n",
        "    start_y = (h - new_h) // 2\n",
        "    cropped = frame[start_y:start_y+new_h, start_x:start_x+new_w]\n",
        "    return cropped\n",
        "\n",
        "def horizontal_flip(frame):\n",
        "    \"\"\"Flip the frame horizontally.\"\"\"\n",
        "    return cv2.flip(frame, 1)\n",
        "\n",
        "def up_sample_video(frames, replicate_fraction=0.5):\n",
        "    \"\"\"\n",
        "    Duplicate a subset of frames to increase the total frame count.\n",
        "    replicate_fraction: fraction of original frames to duplicate.\n",
        "    Final count will be roughly original_frames * (1 + replicate_fraction).\n",
        "    \"\"\"\n",
        "    n_frames = len(frames)\n",
        "    replicate_count = int(n_frames * replicate_fraction)\n",
        "    # Uniformly select frames to duplicate\n",
        "    indices = np.linspace(0, n_frames - 1, replicate_count, dtype=int)\n",
        "    augmented = frames.copy()\n",
        "    # Insert duplicates immediately after the selected frames (in reverse order)\n",
        "    for idx in sorted(indices, reverse=True):\n",
        "        augmented.insert(idx + 1, frames[idx])\n",
        "    return augmented\n",
        "\n",
        "def down_sample_video(frames, drop_fraction=0.35):\n",
        "    \"\"\"\n",
        "    Uniformly drop a fraction of frames so that the final count is (1 - drop_fraction)\n",
        "    times the original.\n",
        "    \"\"\"\n",
        "    n_frames = len(frames)\n",
        "    keep_count = int(n_frames * (1 - drop_fraction))\n",
        "    indices = np.linspace(0, n_frames - 1, keep_count, dtype=int)\n",
        "    return [frames[i] for i in indices]\n",
        "\n",
        "# ---------- Video Processing Pipeline ----------\n",
        "\n",
        "def read_video_frames(video_path):\n",
        "    \"\"\"Read all frames from the video and return them as a list.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def write_video(frames, output_path, fps=30):\n",
        "    \"\"\"Write a list of frames to a video file using OpenCV.\"\"\"\n",
        "    if not frames:\n",
        "        return\n",
        "    h, w, _ = frames[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "    for frame in frames:\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "\n",
        "def process_video_augmentations(video_path, output_dir):\n",
        "    \"\"\"\n",
        "    Given an input video, apply a set of augmentations and save each result.\n",
        "    The output files are saved with suffixes indicating the augmentation type.\n",
        "    \"\"\"\n",
        "    # Read original frames\n",
        "    frames = read_video_frames(video_path)\n",
        "    if not frames:\n",
        "        print(f\"Warning: No frames found in {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get the base name and create an output base filename\n",
        "    base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "    # 1. Save Original for reference\n",
        "    write_video(frames, os.path.join(output_dir, f\"{base_name}_orig.mp4\"))\n",
        "\n",
        "    # 2. Center Crop: apply center crop to each frame\n",
        "    cropped_frames = [center_crop(frame) for frame in frames]\n",
        "    write_video(cropped_frames, os.path.join(output_dir, f\"{base_name}_crop.mp4\"))\n",
        "\n",
        "    # 3. Horizontal Flip: apply flip to each frame\n",
        "    flipped_frames = [horizontal_flip(frame) for frame in frames]\n",
        "    write_video(flipped_frames, os.path.join(output_dir, f\"{base_name}_flip.mp4\"))\n",
        "\n",
        "    # 4. Up-sample: duplicate 50% of frames uniformly\n",
        "    # upsampled_frames = up_sample_video(frames, replicate_fraction=0.5)\n",
        "    # write_video(upsampled_frames, os.path.join(output_dir, f\"{base_name}_up.mp4\"))\n",
        "\n",
        "    # 5. Down-sample: drop 35% of frames uniformly\n",
        "    downsampled_frames = down_sample_video(frames, drop_fraction=0.35)\n",
        "    write_video(downsampled_frames, os.path.join(output_dir, f\"{base_name}_down.mp4\"))\n",
        "\n",
        "def process_folder(input_root, output_root):\n",
        "    \"\"\"\n",
        "    Traverse the folder structure (e.g., Greeting/ALright, Greeting/Hello, etc.),\n",
        "    apply augmentations to each .mov video, and save the augmented videos.\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(input_root):\n",
        "        for file in files:\n",
        "            # Process only .mov files (case-insensitive)\n",
        "            if file.lower().endswith(\".mov\"):\n",
        "                video_path = os.path.join(root, file)\n",
        "                # Create an output subfolder mirroring the input structure\n",
        "                rel_path = os.path.relpath(root, input_root)\n",
        "                output_dir = os.path.join(output_root, rel_path)\n",
        "                os.makedirs(output_dir, exist_ok=True)\n",
        "                print(f\"Processing {video_path}...\")\n",
        "                process_video_augmentations(video_path, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the input root folder (e.g., \"Greeting\") and output root folder.\n",
        "    input_root = \"/content/drive/MyDrive/AISC/Greetings\"\n",
        "    output_root = \"/content/drive/MyDrive/AISC/Greeting_Augmented\"\n",
        "    process_folder(input_root, output_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Implementation\n"
      ],
      "metadata": {
        "id": "rawIrW81E4G_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v98BzD5QMdT1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5e52c4d3913c4abc907d417ed59f8489",
            "f95844d9542a468fb39a04c90a586b56",
            "62c301d67e79410b966590905384b6ba",
            "83f3a35d6b6344ea849a43c16b2d8bd8",
            "0c1fa7c08fd948a9af2de05c468d6609",
            "7d7fc3b85c2a45d09cbf0318f751393d",
            "41eaffe86fd545dab007687c0da209ba",
            "0e80689fffde4655961d387c2640ef94",
            "80061bb5243e4346bd6ac678fb6be688",
            "60fcc430d5934776a06c071e93dfb205",
            "57620310f3494b3aabed296782070c5a"
          ]
        },
        "outputId": "4da267e2-36af-44cf-884a-8a2d4d083ade"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Videos:   0%|          | 0/447 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e52c4d3913c4abc907d417ed59f8489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model to /usr/local/lib/python3.11/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "# -- Functions for keypoint extraction --\n",
        "\n",
        "def process_landmarks(landmarks):\n",
        "    x_list, y_list = [], []\n",
        "    for landmark in landmarks.landmark:\n",
        "        x_list.append(landmark.x)\n",
        "        y_list.append(landmark.y)\n",
        "    return x_list, y_list\n",
        "\n",
        "def process_hand_keypoints(results):\n",
        "    hand1_x, hand1_y, hand2_x, hand2_y = [], [], [], []\n",
        "    if results.multi_hand_landmarks is not None:\n",
        "        if len(results.multi_hand_landmarks) > 0:\n",
        "            hand1 = results.multi_hand_landmarks[0]\n",
        "            hand1_x, hand1_y = process_landmarks(hand1)\n",
        "        if len(results.multi_hand_landmarks) > 1:\n",
        "            hand2 = results.multi_hand_landmarks[1]\n",
        "            hand2_x, hand2_y = process_landmarks(hand2)\n",
        "    return hand1_x, hand1_y, hand2_x, hand2_y\n",
        "\n",
        "def process_pose_keypoints(results):\n",
        "    pose = results.pose_landmarks\n",
        "    if pose: # Only process if pose is not None\n",
        "        pose_x, pose_y = process_landmarks(pose)\n",
        "        return pose_x, pose_y\n",
        "    else:\n",
        "        # Return empty lists or NaN values if no pose is detected\n",
        "        return [np.nan] * 25, [np.nan] * 25\n",
        "\n",
        "def swap_hands(left_wrist, right_wrist, hand, input_hand):\n",
        "    left_wrist_x, left_wrist_y = left_wrist\n",
        "    right_wrist_x, right_wrist_y = right_wrist\n",
        "    hand_x, hand_y = hand\n",
        "    left_dist = (left_wrist_x - hand_x) ** 2 + (left_wrist_y - hand_y) ** 2\n",
        "    right_dist = (right_wrist_x - hand_x) ** 2 + (right_wrist_y - hand_y) ** 2\n",
        "    if left_dist < right_dist and input_hand == \"h2\":\n",
        "        return True\n",
        "    if right_dist < left_dist and input_hand == \"h1\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def process_video(path, save_dir):\n",
        "    \"\"\"\n",
        "    Process a video to extract keypoints and save them in a JSON file.\n",
        "    The label is inferred from the parent folder name.\n",
        "    \"\"\"\n",
        "    hands = mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "    # Increase model_complexity if needed (here set to 2)\n",
        "    pose = mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=2)\n",
        "\n",
        "    pose_points_x, pose_points_y = [], []\n",
        "    hand1_points_x, hand1_points_y = [], []\n",
        "    hand2_points_x, hand2_points_y = [], []\n",
        "\n",
        "    # The label is assumed to be the parent folder's name.\n",
        "    label = os.path.basename(os.path.dirname(path))\n",
        "    label = \"\".join([i for i in label if i.isalpha()]).lower()\n",
        "    uid = os.path.splitext(os.path.basename(path))[0]\n",
        "    uid = \"_\".join([label, uid])\n",
        "    n_frames = 0\n",
        "\n",
        "    if not os.path.isfile(path):\n",
        "        warnings.warn(path + \" file not found\")\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        hand_results = hands.process(image)\n",
        "        pose_results = pose.process(image)\n",
        "\n",
        "        hand1_x, hand1_y, hand2_x, hand2_y = process_hand_keypoints(hand_results)\n",
        "        pose_x, pose_y = process_pose_keypoints(pose_results)\n",
        "\n",
        "        # Swap hands if needed based on wrist distances\n",
        "        if len(hand1_x) > 0 and len(hand2_x) == 0:\n",
        "            if swap_hands(\n",
        "                left_wrist=(pose_x[15], pose_y[15]),\n",
        "                right_wrist=(pose_x[16], pose_y[16]),\n",
        "                hand=(hand1_x[0], hand1_y[0]),\n",
        "                input_hand=\"h1\",\n",
        "            ):\n",
        "                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n",
        "        elif len(hand1_x) == 0 and len(hand2_x) > 0:\n",
        "            if swap_hands(\n",
        "                left_wrist=(pose_x[15], pose_y[15]),\n",
        "                right_wrist=(pose_x[16], pose_y[16]),\n",
        "                hand=(hand2_x[0], hand2_y[0]),\n",
        "                input_hand=\"h2\",\n",
        "            ):\n",
        "                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n",
        "\n",
        "        # Set missing keypoints to NaN for later interpolation.\n",
        "        pose_x = pose_x if pose_x else [np.nan] * 25\n",
        "        pose_y = pose_y if pose_y else [np.nan] * 25\n",
        "        hand1_x = hand1_x if hand1_x else [np.nan] * 21\n",
        "        hand1_y = hand1_y if hand1_y else [np.nan] * 21\n",
        "        hand2_x = hand2_x if hand2_x else [np.nan] * 21\n",
        "        hand2_y = hand2_y if hand2_y else [np.nan] * 21\n",
        "\n",
        "        pose_points_x.append(pose_x)\n",
        "        pose_points_y.append(pose_y)\n",
        "        hand1_points_x.append(hand1_x)\n",
        "        hand1_points_y.append(hand1_y)\n",
        "        hand2_points_x.append(hand2_x)\n",
        "        hand2_points_y.append(hand2_y)\n",
        "\n",
        "        n_frames += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure that at least one frame is processed.\n",
        "    pose_points_x = pose_points_x if pose_points_x else [[np.nan] * 25]\n",
        "    pose_points_y = pose_points_y if pose_points_y else [[np.nan] * 25]\n",
        "    hand1_points_x = hand1_points_x if hand1_points_x else [[np.nan] * 21]\n",
        "    hand1_points_y = hand1_points_y if hand1_points_y else [[np.nan] * 21]\n",
        "    hand2_points_x = hand2_points_x if hand2_points_x else [[np.nan] * 21]\n",
        "    hand2_points_y = hand2_points_y if hand2_points_y else [[np.nan] * 21]\n",
        "\n",
        "    # Prepare the JSON data.\n",
        "    save_data = {\n",
        "        \"uid\": uid,\n",
        "        \"label\": label,\n",
        "        \"pose_x\": pose_points_x,\n",
        "        \"pose_y\": pose_points_y,\n",
        "        \"hand1_x\": hand1_points_x,\n",
        "        \"hand1_y\": hand1_points_y,\n",
        "        \"hand2_x\": hand2_points_x,\n",
        "        \"hand2_y\": hand2_points_y,\n",
        "        \"n_frames\": n_frames,\n",
        "    }\n",
        "    # Save the JSON file. Each video produces one JSON file.\n",
        "    json_filename = f\"{uid}.json\"\n",
        "    json_path = os.path.join(save_dir, json_filename)\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(save_data, f)\n",
        "\n",
        "    hands.close()\n",
        "    pose.close()\n",
        "    del hands, pose, save_data\n",
        "    gc.collect()\n",
        "\n",
        "def process_all_videos(input_root, output_dir):\n",
        "    \"\"\"\n",
        "    Walk through the folder structure (e.g., Greeting/...) and process all videos sequentially.\n",
        "    Each JSON is saved in the output_dir.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    video_paths = []\n",
        "    # Walk through the root folder recursively.\n",
        "    for root, dirs, files in os.walk(input_root):\n",
        "        for file in files:\n",
        "            # Process only video files with extensions .mp4, .avi, .mov\n",
        "            if file.lower().endswith((\".mp4\",\".mov\")):\n",
        "                video_paths.append(os.path.join(root, file))\n",
        "    # Process videos one by one\n",
        "    for path in tqdm(video_paths, desc=\"Processing Videos\"):\n",
        "        process_video(path, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the main folder (e.g., \"Greeting\") and output directory.\n",
        "    input_root = \"/content/drive/MyDrive/AISC/Greeting_Augmented\"\n",
        "    # Set output_dir as a folder, not a file\n",
        "    output_dir = \"/content/drive/MyDrive/AISC/json_files\"\n",
        "    process_all_videos(input_root, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def fix_length(lst, expected):\n",
        "    return lst[:expected] + [np.nan] * (expected - len(lst)) if len(lst) < expected else lst[:expected]\n",
        "\n",
        "def load_feature_vector(json_file, target_frames=200):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    per_frame_dim = 134  # Total keypoints per frame\n",
        "    current_frames = len(data.get(\"pose_x\", []))\n",
        "    frames = []\n",
        "\n",
        "    for i in range(current_frames):\n",
        "        pose_x = fix_length(data.get(\"pose_x\", [])[i], 25) if i < current_frames else [np.nan] * 25\n",
        "        pose_y = fix_length(data.get(\"pose_y\", [])[i], 25) if i < current_frames else [np.nan] * 25\n",
        "        hand1_x = fix_length(data.get(\"hand1_x\", [])[i], 21) if i < current_frames else [np.nan] * 21\n",
        "        hand1_y = fix_length(data.get(\"hand1_y\", [])[i], 21) if i < current_frames else [np.nan] * 21\n",
        "        hand2_x = fix_length(data.get(\"hand2_x\", [])[i], 21) if i < current_frames else [np.nan] * 21\n",
        "        hand2_y = fix_length(data.get(\"hand2_y\", [])[i], 21) if i < current_frames else [np.nan] * 21\n",
        "        frames.append(np.array(pose_x + pose_y + hand1_x + hand1_y + hand2_x + hand2_y, dtype=np.float32))\n",
        "\n",
        "    if not frames:\n",
        "        frames = [np.zeros(per_frame_dim, dtype=np.float32) for _ in range(target_frames)]\n",
        "    elif len(frames) < target_frames:\n",
        "        frames.extend([np.zeros(per_frame_dim, dtype=np.float32)] * (target_frames - len(frames)))\n",
        "    else:\n",
        "        frames = [frames[i] for i in np.linspace(0, len(frames) - 1, target_frames, dtype=int)]\n",
        "\n",
        "    feature_vector = np.concatenate(frames)\n",
        "    if feature_vector.shape[0] != target_frames * per_frame_dim:\n",
        "        raise ValueError(f\"Feature vector length mismatch in {json_file}\")\n",
        "\n",
        "    return feature_vector, data[\"label\"]\n",
        "\n",
        "def load_dataset(json_dir, target_frames=200):\n",
        "    X, y = [], []\n",
        "    for file in os.listdir(json_dir):\n",
        "        filepath = os.path.join(json_dir, file)\n",
        "        if file.endswith(\".json\") and os.path.isfile(filepath):\n",
        "            feature_vector, label = load_feature_vector(filepath, target_frames)\n",
        "            X.append(feature_vector)\n",
        "            y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    json_dir = \"/content/drive/MyDrive/AISC/json_files\"\n",
        "    target_frames = 200\n",
        "    X, y = load_dataset(json_dir, target_frames)\n",
        "\n",
        "    unique_labels = sorted(set(y))\n",
        "    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y_int = np.array([label_to_int[label] for label in y])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_int, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
        "\n",
        "    params = {\n",
        "        'max_depth': 5,\n",
        "        'eta': 0.1,\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': len(unique_labels),\n",
        "        'verbosity': 1\n",
        "    }\n",
        "\n",
        "    model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "    preds = model.predict(dtest)\n",
        "    print(\"Test accuracy:\", accuracy_score(y_test, preds))\n",
        "\n",
        "    model.save_model(\"xgboost_slr.model\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX27OmcY0XzC",
        "outputId": "778eb43d-9ae2-4046-8e92-4e512b7962ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8888888888888888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:53:47] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second implementation"
      ],
      "metadata": {
        "id": "irplCDIvE9XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
        "!cd openpose/\n",
        "!git submodule update --init --recursive --remote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RQKPT5ZHe83",
        "outputId": "ce40c3fe-093c-4e2d-f106-837ca9ff46fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'openpose'...\n",
            "remote: Enumerating objects: 16156, done.\u001b[K\n",
            "remote: Total 16156 (delta 0), reused 0 (delta 0), pack-reused 16156 (from 1)\u001b[K\n",
            "Receiving objects: 100% (16156/16156), 84.46 MiB | 16.69 MiB/s, done.\n",
            "Resolving deltas: 100% (11324/11324), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Assuming you have helper functions for:\n",
        "# - extract_keypoints: using a pre-trained OpenPose to extract key-points for each frame.\n",
        "# - optical_flow_impute: to perform Lucas-Kanade optical flow imputation if needed.\n",
        "# For demonstration, these functions are represented as placeholders.\n",
        "\n",
        "def extract_keypoints(frame):\n",
        "    \"\"\"\n",
        "    Placeholder function to extract keypoints using a pre-trained OpenPose model.\n",
        "    Should return a numpy array of shape (num_points,) where num_points=96.\n",
        "    \"\"\"\n",
        "    # TODO: Implement the actual extraction using OpenPose.\n",
        "    return np.random.rand(96)  # Dummy data for illustration\n",
        "\n",
        "def optical_flow_impute(prev_frame, curr_frame, prev_keypoints):\n",
        "    \"\"\"\n",
        "    Placeholder function for Lucas-Kanade optical flow to impute missing keypoints.\n",
        "    \"\"\"\n",
        "    # TODO: Implement optical flow imputation if required.\n",
        "    # For now, simply return previous keypoints.\n",
        "    return prev_keypoints\n",
        "\n",
        "def extract_features_from_video(video_path, num_frames=200):\n",
        "    \"\"\"\n",
        "    Process a video to extract keypoints from each frame.\n",
        "    Pads with zeros if the video has fewer than num_frames frames.\n",
        "    Returns a flattened feature vector of size 96 * num_frames.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames_features = []\n",
        "    prev_frame = None\n",
        "    prev_keypoints = None\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened() and frame_count < num_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Preprocess the frame if needed (resize, grayscale, etc.)\n",
        "        keypoints = extract_keypoints(frame)\n",
        "\n",
        "        # Check if keypoints are valid (you can define a threshold or condition)\n",
        "        if np.sum(keypoints) == 0 and prev_keypoints is not None:\n",
        "            keypoints = optical_flow_impute(prev_frame, frame, prev_keypoints)\n",
        "\n",
        "        frames_features.append(keypoints)\n",
        "        prev_frame = frame.copy()\n",
        "        prev_keypoints = keypoints\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # If video has fewer than num_frames frames, pad with zeros\n",
        "    while len(frames_features) < num_frames:\n",
        "        frames_features.append(np.zeros(96))\n",
        "\n",
        "    # Convert list to numpy array and flatten\n",
        "    feature_matrix = np.array(frames_features)  # shape: (num_frames, 96)\n",
        "    flattened_features = feature_matrix.flatten()  # shape: (num_frames * 96,)\n",
        "    return flattened_features\n",
        "\n",
        "# Custom transformer to integrate our feature extraction into a pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class VideoFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, num_frames=200):\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"\n",
        "        X is expected to be an array-like of video file paths.\n",
        "        Returns a 2D numpy array with each row being the flattened feature vector.\n",
        "        \"\"\"\n",
        "        features = [extract_features_from_video(video_path, self.num_frames) for video_path in X]\n",
        "        return np.array(features)\n",
        "\n",
        "# Build the pipeline:\n",
        "# Step 1: Feature extraction (including flattening)\n",
        "# Step 2: Standard Scaling (Normalization)\n",
        "# Step 3: XGBoost classifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('feature_extractor', VideoFeatureExtractor(num_frames=200)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=200,    # number of gradient boosted trees\n",
        "        max_depth=5,         # maximum tree depth\n",
        "        learning_rate=0.1,   # learning rate\n",
        "        base_score=0.5,      # initial prediction score\n",
        "        use_label_encoder=False,  # to suppress a warning in recent versions\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Example usage:\n",
        "# Let's assume X_train is a list of video file paths and y_train are the corresponding labels.\n",
        "# X_train = ['video1.mp4', 'video2.mp4', ...]\n",
        "# y_train = [0, 1, ...]  # numerical labels for the signs\n",
        "\n",
        "# Fit the pipeline\n",
        "# pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on new videos\n",
        "# predictions = pipeline.predict(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# Feature Importance Analysis:\n",
        "# --------------------------\n",
        "\n",
        "def compute_feature_importance(pipeline):\n",
        "    \"\"\"\n",
        "    Given a fitted pipeline, extract the XGBoost model and compute feature importances.\n",
        "    We then average the gain for specific keypoint groups (hands and arms).\n",
        "    \"\"\"\n",
        "    # Get the XGBoost model\n",
        "    xgb_model = pipeline.named_steps['classifier']\n",
        "\n",
        "    # Get the feature importances (gain)\n",
        "    importance_dict = xgb_model.get_booster().get_score(importance_type='gain')\n",
        "\n",
        "    # Convert importance_dict keys to indices (assuming feature naming convention 'f0', 'f1', ...)\n",
        "    importance_array = np.zeros(200 * 96)\n",
        "    for k, gain in importance_dict.items():\n",
        "        idx = int(k[1:])  # remove 'f' and convert to integer\n",
        "        importance_array[idx] = gain\n",
        "\n",
        "    # Reshape to (num_frames, 96) to analyze per frame\n",
        "    importance_matrix = importance_array.reshape((200, 96))\n",
        "\n",
        "    # Assume keypoints indices:\n",
        "    #  - Hand keypoints: indices 48 to 95 (48 keypoints per frame for both hands)\n",
        "    #  - Arm keypoints: indices 0 to 47 (shoulder, elbow, wrist, etc.)\n",
        "    hand_importance = importance_matrix[:, 48:96].mean()\n",
        "    arm_importance = importance_matrix[:, 0:48].mean()\n",
        "\n",
        "    print(f\"Average Gain for Hand Keypoints: {hand_importance}\")\n",
        "    print(f\"Average Gain for Arm Keypoints: {arm_importance}\")\n",
        "\n",
        "    return importance_matrix\n",
        "\n",
        "# After training the model, you can call:\n",
        "# feature_importance_matrix = compute_feature_importance(pipeline)\n"
      ],
      "metadata": {
        "id": "bJyWw7zqFCLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import multiprocessing\n",
        "import argparse\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import numpy as np\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "def process_landmarks(landmarks):\n",
        "    x_list, y_list = [], []\n",
        "    for landmark in landmarks.landmark:\n",
        "        x_list.append(landmark.x)\n",
        "        y_list.append(landmark.y)\n",
        "    return x_list, y_list\n",
        "\n",
        "def process_hand_keypoints(results):\n",
        "    hand1_x, hand1_y, hand2_x, hand2_y = [], [], [], []\n",
        "\n",
        "    if results.multi_hand_landmarks is not None:\n",
        "        if len(results.multi_hand_landmarks) > 0:\n",
        "            hand1 = results.multi_hand_landmarks[0]\n",
        "            hand1_x, hand1_y = process_landmarks(hand1)\n",
        "\n",
        "        if len(results.multi_hand_landmarks) > 1:\n",
        "            hand2 = results.multi_hand_landmarks[1]\n",
        "            hand2_x, hand2_y = process_landmarks(hand2)\n",
        "\n",
        "    return hand1_x, hand1_y, hand2_x, hand2_y\n",
        "\n",
        "def process_pose_keypoints(results):\n",
        "    if results.pose_landmarks:  # Check if pose landmarks are detected\n",
        "        pose = results.pose_landmarks\n",
        "        pose_x, pose_y = process_landmarks(pose)\n",
        "        return pose_x, pose_y\n",
        "    else:\n",
        "        return [], []  # Return empty lists if no pose detected\n",
        "\n",
        "def swap_hands(left_wrist, right_wrist, hand, input_hand):\n",
        "    left_wrist_x, left_wrist_y = left_wrist\n",
        "    right_wrist_x, right_wrist_y = right_wrist\n",
        "    hand_x, hand_y = hand\n",
        "\n",
        "    left_dist = (left_wrist_x - hand_x) ** 2 + (left_wrist_y - hand_y) ** 2\n",
        "    right_dist = (right_wrist_x - hand_x) ** 2 + (right_wrist_y - hand_y) ** 2\n",
        "\n",
        "    if left_dist < right_dist and input_hand == \"h2\":\n",
        "        return True\n",
        "\n",
        "    if right_dist < left_dist and input_hand == \"h1\":\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def process_video(path, save_dir):\n",
        "    hands = mp.solutions.hands.Hands(\n",
        "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
        "    )\n",
        "    # Removed the 'upper_body_only' parameter as it's no longer supported.\n",
        "    pose = mp.solutions.pose.Pose(\n",
        "        min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
        "    )\n",
        "\n",
        "    pose_points_x, pose_points_y = [], []\n",
        "    hand1_points_x, hand1_points_y = [], []\n",
        "    hand2_points_x, hand2_points_y = [], []\n",
        "\n",
        "    # Get label from parent folder name\n",
        "    label = os.path.basename(os.path.dirname(path))\n",
        "    label = \"\".join([i for i in label if i.isalpha()]).lower()\n",
        "    uid = os.path.splitext(os.path.basename(path))[0]\n",
        "    uid = \"_\".join([label, uid])\n",
        "    n_frames = 0\n",
        "    if not os.path.isfile(path):\n",
        "        warnings.warn(path + \" file not found: \" + path)\n",
        "        return  # Skip processing if file not found\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        warnings.warn(f\"Error opening video file: {path}\")\n",
        "        return\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, image = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        hand_results = hands.process(image)\n",
        "        pose_results = pose.process(image)\n",
        "\n",
        "        hand1_x, hand1_y, hand2_x, hand2_y = process_hand_keypoints(hand_results)\n",
        "        pose_x, pose_y = process_pose_keypoints(pose_results)\n",
        "\n",
        "        ## Assign hands to correct positions\n",
        "        if len(hand1_x) > 0 and len(hand2_x) == 0 and len(pose_x) > 16 and len(pose_y) > 16:\n",
        "            if swap_hands(\n",
        "                left_wrist=(pose_x[15], pose_y[15]),\n",
        "                right_wrist=(pose_x[16], pose_y[16]),\n",
        "                hand=(hand1_x[0], hand1_y[0]),\n",
        "                input_hand=\"h1\",\n",
        "            ):\n",
        "                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n",
        "\n",
        "        elif len(hand1_x) == 0 and len(hand2_x) > 0 and len(pose_x) > 16 and len(pose_y) > 16:\n",
        "            if swap_hands(\n",
        "                left_wrist=(pose_x[15], pose_y[15]),\n",
        "                right_wrist=(pose_x[16], pose_y[16]),\n",
        "                hand=(hand2_x[0], hand2_y[0]),\n",
        "                input_hand=\"h2\",\n",
        "            ):\n",
        "                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n",
        "\n",
        "        ## Set to nan so that values can be interpolated in dataloader\n",
        "        pose_x = pose_x if pose_x else [np.nan] * 25\n",
        "        pose_y = pose_y if pose_y else [np.nan] * 25\n",
        "\n",
        "        hand1_x = hand1_x if hand1_x else [np.nan] * 21\n",
        "        hand1_y = hand1_y if hand1_y else [np.nan] * 21\n",
        "        hand2_x = hand2_x if hand2_x else [np.nan] * 21\n",
        "        hand2_y = hand2_y if hand2_y else [np.nan] * 21\n",
        "\n",
        "        pose_points_x.append(pose_x)\n",
        "        pose_points_y.append(pose_y)\n",
        "        hand1_points_x.append(hand1_x)\n",
        "        hand1_points_y.append(hand1_y)\n",
        "        hand2_points_x.append(hand2_x)\n",
        "        hand2_points_y.append(hand2_y)\n",
        "\n",
        "        n_frames += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    ## Set to nan so that values can be interpolated in dataloader\n",
        "    pose_points_x = pose_points_x if pose_points_x else [[np.nan] * 25]\n",
        "    pose_points_y = pose_points_y if pose_points_y else [[np.nan] * 25]\n",
        "\n",
        "    hand1_points_x = hand1_points_x if hand1_points_x else [[np.nan] * 21]\n",
        "    hand1_points_y = hand1_points_y if hand1_points_y else [[np.nan] * 21]\n",
        "    hand2_points_x = hand2_points_x if hand2_points_x else [[np.nan] * 21]\n",
        "    hand2_points_y = hand2_points_y if hand2_points_y else [[np.nan] * 21]\n",
        "\n",
        "    save_data = {\n",
        "        \"uid\": uid,\n",
        "        \"label\": label,\n",
        "        \"pose_x\": pose_points_x,\n",
        "        \"pose_y\": pose_points_y,\n",
        "        \"hand1_x\": hand1_points_x,\n",
        "        \"hand1_y\": hand1_points_y,\n",
        "        \"hand2_x\": hand2_points_x,\n",
        "        \"hand2_y\": hand2_points_y,\n",
        "        \"n_frames\": n_frames,\n",
        "    }\n",
        "    with open(os.path.join(save_dir, f\"{uid}.json\"), \"w\") as f:\n",
        "        json.dump(save_data, f)\n",
        "\n",
        "    hands.close()\n",
        "    pose.close()\n",
        "    del hands, pose, save_data\n",
        "    gc.collect()\n",
        "\n",
        "def get_all_video_paths(root_dir, extensions=(\".mov\", \".mp4\")):\n",
        "    \"\"\"\n",
        "    Recursively collect all video file paths in root_dir that match the given extensions.\n",
        "    \"\"\"\n",
        "    video_paths = []\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(extensions):\n",
        "                video_paths.append(os.path.join(dirpath, filename))\n",
        "    return video_paths\n",
        "\n",
        "def save_keypoints(dataset, file_paths, mode, save_dir_base):\n",
        "    save_dir = os.path.join(save_dir_base, f\"{dataset}_{mode}_keypoints\")\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    Parallel(n_jobs=n_cores, backend=\"multiprocessing\")(\n",
        "        delayed(process_video)(path, save_dir)\n",
        "        for path in tqdm(file_paths, desc=f\"processing {mode} videos\")\n",
        "    )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser(description=\"Generate keypoints from Mediapipe for Hands and Upper Body\")\n",
        "#     parser.add_argument(\n",
        "#         \"--include_dir\",\n",
        "#         default=\"/content/drive/MyDrive/AISC/Greeting_Augmented\",\n",
        "#         type=str,\n",
        "#         help=\"Path to the location of videos\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--save_dir\",\n",
        "#         default=\"/content/drive/MyDrive/AISC/keypoints_output\",\n",
        "#         type=str,\n",
        "#         help=\"Location to output json keypoint files\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--dataset\", default=\"greeting_augmented\", type=str, help=\"Dataset name\"\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"--mode\", default=\"all\", type=str, help=\"Mode to process (set to 'all' to process all videos)\"\n",
        "#     )\n",
        "\n",
        "#     # Use parse_known_args to ignore extra arguments passed by Colab/Jupyter.\n",
        "#     args, unknown = parser.parse_known_args()\n",
        "\n",
        "#     n_cores = multiprocessing.cpu_count()\n",
        "\n",
        "#     # Recursively collect all video files from the include_dir\n",
        "#     video_paths = get_all_video_paths(args.include_dir)\n",
        "#     if not video_paths:\n",
        "#         print(\"No video files found in\", args.include_dir)\n",
        "#     else:\n",
        "#         print(f\"Found {len(video_paths)} video files. Processing...\")\n",
        "#         save_keypoints(args.dataset, video_paths, args.mode, args.save_dir)\n",
        "\n",
        "#     print(\"Keypoint extraction complete! Keypoints saved in:\", args.save_dir)\n"
      ],
      "metadata": {
        "id": "9uy_yAAeQm8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "keypoints_dir = \"/content/drive/MyDrive/AISC/keypoints_output/greeting_augmented_all_keypoints\"\n",
        "all_json_files = sorted(glob.glob(os.path.join(keypoints_dir, \"*.json\")))\n",
        "print(\"Total JSON files:\", len(all_json_files))\n",
        "\n",
        "def extract_label_from_filename(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    label_str = filename.split('_')[0]\n",
        "    label_str = \"\".join([c for c in label_str if c.isalpha()]).lower()\n",
        "    return label_str\n",
        "\n",
        "# Example:\n",
        "labels_str = [extract_label_from_filename(f) for f in all_json_files]\n",
        "unique_labels = sorted(set(labels_str))\n",
        "print(\"Unique labels found:\", unique_labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlLi1USoYsjU",
        "outputId": "baa2fb9b-2712-449d-ec71-903ec09cdd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total JSON files: 447\n",
            "Unique labels found: ['alright', 'goodafternoon', 'goodmorning', 'hello', 'howareyou']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Shuffle data\n",
        "rng = np.random.default_rng(seed=42)\n",
        "indices = np.arange(len(all_json_files))\n",
        "rng.shuffle(indices)\n",
        "\n",
        "# Define split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio   = 0.15\n",
        "test_ratio  = 0.15\n",
        "\n",
        "train_end = int(train_ratio * len(indices))\n",
        "val_end   = int((train_ratio + val_ratio) * len(indices))\n",
        "\n",
        "train_indices = indices[:train_end]\n",
        "val_indices   = indices[train_end:val_end]\n",
        "test_indices  = indices[val_end:]\n",
        "\n",
        "train_files  = [all_json_files[i] for i in train_indices]\n",
        "val_files    = [all_json_files[i] for i in val_indices]\n",
        "test_files   = [all_json_files[i] for i in test_indices]\n",
        "\n",
        "train_labels_str = [labels_str[i] for i in train_indices]\n",
        "val_labels_str   = [labels_str[i] for i in val_indices]\n",
        "test_labels_str  = [labels_str[i] for i in test_indices]\n",
        "\n",
        "print(\"Train set size:\", len(train_files))\n",
        "print(\"Val set size:  \", len(val_files))\n",
        "print(\"Test set size: \", len(test_files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qKsgYkuIf5O",
        "outputId": "17889356-a612-4482-d756-836ce95e5ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 312\n",
            "Val set size:   67\n",
            "Test set size:  68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    'alright': 0,\n",
        "    'goodafternoon': 1,\n",
        "    'goodmorning': 2,\n",
        "    'hello': 3,\n",
        "    'howareyou': 4\n",
        "}\n",
        "\n",
        "train_labels = np.array([label_map[lbl] for lbl in train_labels_str])\n",
        "val_labels   = np.array([label_map[lbl] for lbl in val_labels_str])\n",
        "test_labels  = np.array([label_map[lbl] for lbl in test_labels_str])\n"
      ],
      "metadata": {
        "id": "VRphJUFsIhgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "NUM_FRAMES = 120\n",
        "FEATURES_PER_FRAME = 134\n",
        "\n",
        "def load_keypoints(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def create_feature_vector(data, num_frames=NUM_FRAMES):\n",
        "    pose_x   = data.get('pose_x', [])\n",
        "    pose_y   = data.get('pose_y', [])\n",
        "    hand1_x  = data.get('hand1_x', [])\n",
        "    hand1_y  = data.get('hand1_y', [])\n",
        "    hand2_x  = data.get('hand2_x', [])\n",
        "    hand2_y  = data.get('hand2_y', [])\n",
        "\n",
        "    total_frames = len(pose_x)\n",
        "    features = []\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        if i < total_frames:\n",
        "            frame_features = (\n",
        "                pose_x[i] + pose_y[i] +\n",
        "                hand1_x[i] + hand1_y[i] +\n",
        "                hand2_x[i] + hand2_y[i]\n",
        "            )\n",
        "            # Pad if needed\n",
        "            frame_features = frame_features[:FEATURES_PER_FRAME] # added\n",
        "            if len(frame_features) < FEATURES_PER_FRAME:\n",
        "                frame_features += [np.nan] * (FEATURES_PER_FRAME - len(frame_features))\n",
        "        else:\n",
        "            frame_features = [np.nan] * FEATURES_PER_FRAME\n",
        "        features.append(frame_features)\n",
        "\n",
        "    return np.array(features, dtype=object).flatten() # modified\n",
        "\n",
        "class KeypointFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, num_frames=NUM_FRAMES):\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        all_features = []\n",
        "        for path in X:\n",
        "            data = load_keypoints(path)\n",
        "            feat_vector = create_feature_vector(data, self.num_frames)\n",
        "            all_features.append(feat_vector)\n",
        "        return np.array(all_features)\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('feature_extractor', KeypointFeatureExtractor(num_frames=NUM_FRAMES)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        base_score=0.5,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "Cs6GfMrqY8ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting model on training data...\")\n",
        "pipeline.fit(train_files, train_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "1t1fmssUI-aQ",
        "outputId": "0093f924-ac08-4408-8714-e4f437df5bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model on training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:39:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('feature_extractor', KeypointFeatureExtractor()),\n",
              "                ('scaler', StandardScaler()),\n",
              "                ('classifier',\n",
              "                 XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric='logloss',\n",
              "                               feature_types=None, gamma=None, gro...icy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.1,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=5, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, multi_strategy=None,\n",
              "                               n_estimators=200, n_jobs=None,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective='multi:softprob', ...))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_extractor&#x27;, KeypointFeatureExtractor()),\n",
              "                (&#x27;scaler&#x27;, StandardScaler()),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
              "                               feature_types=None, gamma=None, gro...icy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.1,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=5, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, multi_strategy=None,\n",
              "                               n_estimators=200, n_jobs=None,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;feature_extractor&#x27;, KeypointFeatureExtractor()),\n",
              "                (&#x27;scaler&#x27;, StandardScaler()),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None, device=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
              "                               feature_types=None, gamma=None, gro...icy=None,\n",
              "                               importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=0.1,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=5, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, multi_strategy=None,\n",
              "                               n_estimators=200, n_jobs=None,\n",
              "                               num_parallel_tree=None,\n",
              "                               objective=&#x27;multi:softprob&#x27;, ...))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KeypointFeatureExtractor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>KeypointFeatureExtractor()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
              "              feature_types=None, gamma=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
              "              n_jobs=None, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div> </div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Evaluating on validation set...\")\n",
        "val_preds = pipeline.predict(val_files)\n",
        "val_acc = accuracy_score(val_labels, val_preds)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(val_labels, val_preds, target_names=list(label_map.keys())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxK9ThNyI_FD",
        "outputId": "c77d0d7e-f770-44a5-b268-bdcd19e79795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on validation set...\n",
            "Validation Accuracy: 0.8507462686567164\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      alright       0.77      0.83      0.80        12\n",
            "goodafternoon       0.86      0.80      0.83        15\n",
            "  goodmorning       1.00      0.91      0.95        11\n",
            "        hello       0.80      0.86      0.83        14\n",
            "    howareyou       0.87      0.87      0.87        15\n",
            "\n",
            "     accuracy                           0.85        67\n",
            "    macro avg       0.86      0.85      0.85        67\n",
            " weighted avg       0.86      0.85      0.85        67\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating on test set...\")\n",
        "test_preds = pipeline.predict(test_files)\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds, target_names=list(label_map.keys())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPf6egpGJeLl",
        "outputId": "3c777215-483a-499b-ae24-ad3f4fdaff17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set...\n",
            "Test Accuracy: 0.8676470588235294\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      alright       0.93      0.72      0.81        18\n",
            "goodafternoon       1.00      0.87      0.93        15\n",
            "  goodmorning       0.90      0.90      0.90        10\n",
            "        hello       0.90      0.95      0.92        19\n",
            "    howareyou       0.55      1.00      0.71         6\n",
            "\n",
            "     accuracy                           0.87        68\n",
            "    macro avg       0.85      0.89      0.85        68\n",
            " weighted avg       0.90      0.87      0.87        68\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "\n",
        "# Save pipeline to disk\n",
        "dump(pipeline, \"/content/drive/MyDrive/AISC/asl_pipeline.joblib\")\n",
        "\n",
        "xgb_clf = pipeline.named_steps['classifier']\n",
        "\n",
        "# Save the booster\n",
        "xgb_clf.get_booster().save_model(\"/content/drive/MyDrive/AISC/xgb_booster.json\")\n"
      ],
      "metadata": {
        "id": "mT0b1QdBNEWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_video_path  = \"/content/drive/MyDrive/AISC/Greeting_Augmented/How are you/MVI_9987_orig.mp4\"\n",
        "output_json_dir = \"/content/drive/MyDrive/AISC/keypoints_output/inference_keypoints\"\n",
        "\n",
        "os.makedirs(output_json_dir, exist_ok=True)\n",
        "process_video(new_video_path, output_json_dir)\n",
        "\n",
        "print(\"Keypoint extraction for new video complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKZT5JP-N5Ot",
        "outputId": "b73c3814-4be8-4868-f42b-579140be0cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keypoint extraction for new video complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Load your saved pipeline (adjust the path if necessary)\n",
        "pipeline = load(\"/content/drive/MyDrive/AISC/asl_pipeline.joblib\")\n",
        "print(\"Pipeline loaded successfully!\")\n",
        "\n",
        "# Define your output folder for inference keypoints\n",
        "output_json_dir = \"/content/drive/MyDrive/AISC/keypoints_output/inference_keypoints\"\n",
        "\n",
        "# Get the JSON file path from the output folder.\n",
        "json_files = sorted(glob.glob(os.path.join(output_json_dir, \"*.json\")))\n",
        "if not json_files:\n",
        "    print(\"No JSON keypoint files found!\")\n",
        "else:\n",
        "    # Labelmap to convert numeric predictions to text\n",
        "    label_map = {\n",
        "        0: 'alright',\n",
        "        1: 'goodafternoon',\n",
        "        2: 'goodmorning',\n",
        "        3: 'hello',\n",
        "        4: 'howareyou'\n",
        "    }\n",
        "\n",
        "    for json_file in json_files:\n",
        "        prediction = pipeline.predict([json_file])[0]\n",
        "        predicted_label = label_map.get(prediction, \"Unknown\")\n",
        "        base_name = os.path.basename(json_file)\n",
        "        real_label = base_name.split('_')[0].lower()\n",
        "        print(f\"File: {base_name}\")\n",
        "        print(f\"Predicted Sign for the video: {predicted_label}\")\n",
        "        print(f\"Real Sign for the video: {real_label}\")\n",
        "        print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY3OKouAQgc8",
        "outputId": "dcac12b3-f8f2-46dd-e1f4-53a943fb45b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline loaded successfully!\n",
            "File: alright_MVI_0044_down.json\n",
            "Predicted Sign for the video: alright\n",
            "Real Sign for the video: alright\n",
            "----------------------------------------\n",
            "File: alright_MVI_0096_crop.json\n",
            "Predicted Sign for the video: alright\n",
            "Real Sign for the video: alright\n",
            "----------------------------------------\n",
            "File: alright_MVI_9924.json\n",
            "Predicted Sign for the video: alright\n",
            "Real Sign for the video: alright\n",
            "----------------------------------------\n",
            "File: alright_MVI_9963_orig.json\n",
            "Predicted Sign for the video: alright\n",
            "Real Sign for the video: alright\n",
            "----------------------------------------\n",
            "File: alright_MVI_9989_crop.json\n",
            "Predicted Sign for the video: alright\n",
            "Real Sign for the video: alright\n",
            "----------------------------------------\n",
            "File: goodafternoon_MVI_0048_down.json\n",
            "Predicted Sign for the video: goodafternoon\n",
            "Real Sign for the video: goodafternoon\n",
            "----------------------------------------\n",
            "File: goodafternoon_MVI_0049.json\n",
            "Predicted Sign for the video: goodafternoon\n",
            "Real Sign for the video: goodafternoon\n",
            "----------------------------------------\n",
            "File: goodafternoon_MVI_0050_ (2)_orig.json\n",
            "Predicted Sign for the video: goodafternoon\n",
            "Real Sign for the video: goodafternoon\n",
            "----------------------------------------\n",
            "File: goodafternoon_MVI_0051_down.json\n",
            "Predicted Sign for the video: goodafternoon\n",
            "Real Sign for the video: goodafternoon\n",
            "----------------------------------------\n",
            "File: goodmorning_MVI_0045.json\n",
            "Predicted Sign for the video: goodmorning\n",
            "Real Sign for the video: goodmorning\n",
            "----------------------------------------\n",
            "File: goodmorning_MVI_0047_orig.json\n",
            "Predicted Sign for the video: goodmorning\n",
            "Real Sign for the video: goodmorning\n",
            "----------------------------------------\n",
            "File: goodmorning_MVI_9932_flip.json\n",
            "Predicted Sign for the video: goodmorning\n",
            "Real Sign for the video: goodmorning\n",
            "----------------------------------------\n",
            "File: goodmorning_MVI_9934_down.json\n",
            "Predicted Sign for the video: goodmorning\n",
            "Real Sign for the video: goodmorning\n",
            "----------------------------------------\n",
            "File: goodmorning_MVI_9934_orig.json\n",
            "Predicted Sign for the video: goodmorning\n",
            "Real Sign for the video: goodmorning\n",
            "----------------------------------------\n",
            "File: hello_MVI_0030_down.json\n",
            "Predicted Sign for the video: hello\n",
            "Real Sign for the video: hello\n",
            "----------------------------------------\n",
            "File: hello_MVI_0038_orig.json\n",
            "Predicted Sign for the video: hello\n",
            "Real Sign for the video: hello\n",
            "----------------------------------------\n",
            "File: hello_MVI_0089_down.json\n",
            "Predicted Sign for the video: hello\n",
            "Real Sign for the video: hello\n",
            "----------------------------------------\n",
            "File: hello_MVI_0091_orig.json\n",
            "Predicted Sign for the video: hello\n",
            "Real Sign for the video: hello\n",
            "----------------------------------------\n",
            "File: hello_MVI_9917_crop.json\n",
            "Predicted Sign for the video: hello\n",
            "Real Sign for the video: hello\n",
            "----------------------------------------\n",
            "File: howareyou_MVI_0036_orig.json\n",
            "Predicted Sign for the video: howareyou\n",
            "Real Sign for the video: howareyou\n",
            "----------------------------------------\n",
            "File: howareyou_MVI_0040_down.json\n",
            "Predicted Sign for the video: howareyou\n",
            "Real Sign for the video: howareyou\n",
            "----------------------------------------\n",
            "File: howareyou_MVI_9961_down.json\n",
            "Predicted Sign for the video: howareyou\n",
            "Real Sign for the video: howareyou\n",
            "----------------------------------------\n",
            "File: howareyou_MVI_9987_orig.json\n",
            "Predicted Sign for the video: howareyou\n",
            "Real Sign for the video: howareyou\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rawIrW81E4G_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e52c4d3913c4abc907d417ed59f8489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f95844d9542a468fb39a04c90a586b56",
              "IPY_MODEL_62c301d67e79410b966590905384b6ba",
              "IPY_MODEL_83f3a35d6b6344ea849a43c16b2d8bd8"
            ],
            "layout": "IPY_MODEL_0c1fa7c08fd948a9af2de05c468d6609"
          }
        },
        "f95844d9542a468fb39a04c90a586b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d7fc3b85c2a45d09cbf0318f751393d",
            "placeholder": "​",
            "style": "IPY_MODEL_41eaffe86fd545dab007687c0da209ba",
            "value": "Processing Videos: 100%"
          }
        },
        "62c301d67e79410b966590905384b6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e80689fffde4655961d387c2640ef94",
            "max": 447,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80061bb5243e4346bd6ac678fb6be688",
            "value": 447
          }
        },
        "83f3a35d6b6344ea849a43c16b2d8bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60fcc430d5934776a06c071e93dfb205",
            "placeholder": "​",
            "style": "IPY_MODEL_57620310f3494b3aabed296782070c5a",
            "value": " 447/447 [1:01:17&lt;00:00,  7.97s/it]"
          }
        },
        "0c1fa7c08fd948a9af2de05c468d6609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d7fc3b85c2a45d09cbf0318f751393d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41eaffe86fd545dab007687c0da209ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e80689fffde4655961d387c2640ef94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80061bb5243e4346bd6ac678fb6be688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60fcc430d5934776a06c071e93dfb205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57620310f3494b3aabed296782070c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}