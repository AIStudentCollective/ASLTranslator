{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU2VPWIb-RVK"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/dataset/train/hello\n",
        "!mkdir -p /content/dataset/train/wrong\n",
        "!mkdir -p /content/dataset/test/hello\n",
        "!mkdir -p /content/dataset/test/wrong\n",
        "\n",
        "!ls hello_*.MOV | shuf -n 16 | xargs -I {} mv {} /content/dataset/test/hello/\n",
        "!mv hello_*.MOV /content/dataset/train/hello/\n",
        "\n",
        "!ls wrong_*.MOV | shuf -n 6 | xargs -I {} mv {} /content/dataset/test/wrong/\n",
        "!mv wrong_*.MOV /content/dataset/train/wrong/"
      ],
      "metadata": {
        "id": "G4bDTJCF_1AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import random\n",
        "\n",
        "class VideoFrameGenerator(Sequence):\n",
        "    def __init__(self, video_folder, batch_size=4, sequence_length=10, target_size=(224, 224), shuffle=True):\n",
        "        self.video_folder = video_folder\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.classes = sorted(os.listdir(video_folder))\n",
        "        self.video_paths = []\n",
        "\n",
        "        # Collect all video file paths and labels\n",
        "        for class_idx, class_name in enumerate(self.classes):\n",
        "            class_folder = os.path.join(video_folder, class_name)\n",
        "            videos = [os.path.join(class_folder, vid) for vid in os.listdir(class_folder)]\n",
        "            self.video_paths.extend([(vid, class_idx) for vid in videos])\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.video_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.video_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_videos = self.video_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self._load_batch(batch_videos)\n",
        "        return X, y\n",
        "\n",
        "    def _load_batch(self, batch_videos):\n",
        "      X_batch = []\n",
        "      y_batch = []\n",
        "\n",
        "      for video_path, label in batch_videos:\n",
        "        frames = self._extract_frames(video_path)\n",
        "\n",
        "        if frames.shape != (self.sequence_length, *self.target_size, 3):\n",
        "            print(f\"Skipping {video_path}: Unexpected shape {frames.shape}\")\n",
        "            continue\n",
        "\n",
        "        X_batch.append(frames)\n",
        "        y_batch.append(label)\n",
        "\n",
        "      return np.array(X_batch), np.array(y_batch)\n",
        "\n",
        "\n",
        "    def _extract_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      frames = []\n",
        "      total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      frame_interval = max(1, total_frames // self.sequence_length)\n",
        "\n",
        "      for i in range(self.sequence_length):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.resize(frame, self.target_size)\n",
        "            frame = frame / 255.0  # Normalize\n",
        "\n",
        "            # Apply Transformations (Data Augmentation)\n",
        "            if random.random() < 0.5:\n",
        "                frame = cv2.flip(frame, 1)  # Random Horizontal Flip\n",
        "            if random.random() < 0.3:\n",
        "                frame = frame * random.uniform(0.7, 1.3)  # Random Brightness Change\n",
        "            if random.random() < 0.3:\n",
        "                noise = np.random.normal(0, 0.1, frame.shape)  # Add Gaussian Noise\n",
        "                frame = np.clip(frame + noise, 0, 1)\n",
        "\n",
        "        else:\n",
        "            frame = np.zeros((*self.target_size, 3))  # Fill missing frames with black images\n",
        "        frames.append(frame)\n",
        "\n",
        "      cap.release()\n",
        "      return np.array(frames)\n",
        "\n",
        "# Initialize generators\n",
        "train_gen = VideoFrameGenerator(\"/content/dataset/train\", batch_size=4, sequence_length=10)\n",
        "val_gen = VideoFrameGenerator(\"/content/dataset/test\", batch_size=4, sequence_length=10)\n"
      ],
      "metadata": {
        "id": "XlXY9A-xAFyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Load pre-trained CNN\n",
        "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Define LSTM Model\n",
        "model = Sequential([\n",
        "    TimeDistributed(base_model, input_shape=(10, 224, 224, 3)),\n",
        "    TimeDistributed(Flatten()),\n",
        "    Dropout(0.4),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dropout(0.4),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_gen, validation_data=val_gen, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sabHKV2OAN5r",
        "outputId": "8946ec02-69c1-4e37-9d9f-c18e88bd2f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 19s/step - accuracy: 0.6215 - loss: 0.9647 - val_accuracy: 0.6500 - val_loss: 0.6331\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 11s/step - accuracy: 0.7688 - loss: 0.5125 - val_accuracy: 0.4500 - val_loss: 0.8687\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 11s/step - accuracy: 0.6653 - loss: 0.7175 - val_accuracy: 0.3000 - val_loss: 0.7782\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 11s/step - accuracy: 0.5889 - loss: 0.7501 - val_accuracy: 0.5500 - val_loss: 0.7220\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 11s/step - accuracy: 0.4403 - loss: 0.8750 - val_accuracy: 0.5500 - val_loss: 0.6808\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 11s/step - accuracy: 0.6299 - loss: 0.5433 - val_accuracy: 0.6000 - val_loss: 0.6751\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 11s/step - accuracy: 0.6465 - loss: 0.5264 - val_accuracy: 0.4500 - val_loss: 0.8092\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 11s/step - accuracy: 0.4382 - loss: 0.7925 - val_accuracy: 0.4000 - val_loss: 0.8636\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 11s/step - accuracy: 0.7319 - loss: 0.5812 - val_accuracy: 0.3000 - val_loss: 0.9679\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 11s/step - accuracy: 0.6924 - loss: 0.6445 - val_accuracy: 0.4000 - val_loss: 0.8735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bf5e44204f0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def predict_video(video_path, model):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_interval = max(1, total_frames // 10)  # Ensure we get 10 frames\n",
        "\n",
        "    for i in range(10):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)\n",
        "        ret, img = cap.read()\n",
        "        if ret:\n",
        "            img = cv2.resize(img, (224, 224))\n",
        "            img = img / 255.0\n",
        "            frames.append(img)\n",
        "        else:\n",
        "            frames.append(np.zeros((224, 224, 3)))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    frames = np.array(frames)\n",
        "    frames = np.expand_dims(frames, axis=0)  # Add batch dimension\n",
        "    prediction = model.predict(frames)\n",
        "    class_index = np.argmax(prediction)\n",
        "\n",
        "    return \"hello\" if class_index == 0 else \"wrong\"\n",
        "\n",
        "\n",
        "# Folder paths\n",
        "test_folder = \"/content/dataset/test\"\n",
        "classes = [\"hello\", \"wrong\"]\n",
        "correct_predictions = 0\n",
        "total_videos = 0\n",
        "\n",
        "# Loop through test videos\n",
        "for label in classes:\n",
        "    class_folder = os.path.join(test_folder, label)\n",
        "    for video in os.listdir(class_folder):\n",
        "        video_path = os.path.join(class_folder, video)\n",
        "        predicted_label = predict_video(video_path, model)\n",
        "\n",
        "        print(f\"Video: {video} | True Label: {label} | Predicted: {predicted_label}\")\n",
        "\n",
        "        if predicted_label == label:\n",
        "            correct_predictions += 1\n",
        "        total_videos += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (correct_predictions / total_videos) * 100\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8wwnfnuEIen",
        "outputId": "42c621e0-01f1-41d8-c76d-2f77f908f21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Video: hello_0019.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Video: hello_0017.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: hello_0021.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: hello_0010.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Video: hello_0016.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Video: .ipynb_checkpoints | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "Video: hello_0011.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: hello_002.MOV | True Label: hello | Predicted: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Video: hello_0018.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Video: hello_0014.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: hello_0020.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Video: hello_0013.MOV | True Label: hello | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Video: wrong_0013.MOV | True Label: wrong | Predicted: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Video: wrong_0016.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: wrong_006.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "Video: wrong_003.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: wrong_0010.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "Video: wrong_001.MOV | True Label: wrong | Predicted: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Video: wrong_0014.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Video: wrong_007.MOV | True Label: wrong | Predicted: wrong\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Video: wrong_0011.MOV | True Label: wrong | Predicted: wrong\n",
            "\n",
            "Model Accuracy on Test Set: 38.10%\n"
          ]
        }
      ]
    }
  ]
}